{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "!pip install jupyterplot\n",
    "from jupyterplot import ProgressPlot\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import time\n",
    "from itertools import count\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and Experience Buffer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADRQN(nn.Module):\n",
    "    def __init__(self, n_actions, state_size, embedding_size):\n",
    "        super(ADRQN, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = nn.Linear(n_actions, embedding_size)\n",
    "        self.obs_layer = nn.Linear(state_size, 16)\n",
    "        self.obs_layer2 = nn.Linear(16,32)\n",
    "        self.lstm = nn.LSTM(input_size = 32+embedding_size, hidden_size = 128, batch_first = True)\n",
    "        self.out_layer = nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self, observation, action, hidden = None):\n",
    "        #Takes observations with shape (batch_size, seq_len, obs_dim)\n",
    "        #Takes one_hot actions with shape (batch_size, seq_len, n_actions)\n",
    "        action_embedded = self.embedder(action)\n",
    "        observation = F.relu(self.obs_layer(observation))\n",
    "        observation = F.relu(self.obs_layer2(observation))\n",
    "        lstm_input = torch.cat([observation, action_embedded], dim = -1)\n",
    "        if hidden is not None:\n",
    "            lstm_out, hidden_out = self.lstm(lstm_input, hidden)\n",
    "        else:\n",
    "            lstm_out, hidden_out = self.lstm(lstm_input)\n",
    "\n",
    "        q_values = self.out_layer(lstm_out)\n",
    "        return q_values, hidden_out\n",
    "    \n",
    "    def act(self, observation, last_action, epsilon, hidden = None):\n",
    "        q_values, hidden_out = self.forward(observation, last_action, hidden)\n",
    "        if np.random.uniform() > epsilon:\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        return action, hidden_out\n",
    "\n",
    "class ExpBuffer():\n",
    "    def __init__(self, max_storage, sample_length):\n",
    "        self.max_storage = max_storage\n",
    "        self.sample_length = sample_length\n",
    "        self.counter = -1\n",
    "        self.filled = -1\n",
    "        self.storage = [0 for i in range(max_storage)]\n",
    "\n",
    "    def write_tuple(self, aoarod):\n",
    "        if self.counter < self.max_storage-1:\n",
    "            self.counter +=1\n",
    "        if self.filled < self.max_storage:\n",
    "            self.filled += 1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "        self.storage[self.counter] = aoarod\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #Returns sizes of (batch_size, seq_len, *) depending on action/observation/return/done\n",
    "        seq_len = self.sample_length\n",
    "        last_actions = []\n",
    "        last_observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        observations = []\n",
    "        dones = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if self.filled - seq_len < 0 :\n",
    "                raise Exception(\"Reduce seq_len or increase exploration at start.\")\n",
    "            start_idx = np.random.randint(self.filled-seq_len)\n",
    "            #print(self.filled)\n",
    "            #print(start_idx)\n",
    "            last_act, last_obs, act, rew, obs, done = zip(*self.storage[start_idx:start_idx+seq_len])\n",
    "            last_actions.append(list(last_act))\n",
    "            last_observations.append(last_obs)\n",
    "            actions.append(list(act))\n",
    "            rewards.append(list(rew))\n",
    "            observations.append(list(obs))\n",
    "            dones.append(list(done))\n",
    "           \n",
    "        return torch.tensor(last_actions).cuda(), torch.tensor(last_observations, dtype = torch.float32).cuda(), torch.tensor(actions).cuda(), torch.tensor(rewards).float().cuda() , torch.tensor(observations, dtype = torch.float32).cuda(), torch.tensor(dones).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "embedding_size = 8\n",
    "M_episodes = 2500\n",
    "replay_buffer_size = 100000\n",
    "sample_length = 20\n",
    "replay_buffer = ExpBuffer(replay_buffer_size, sample_length)\n",
    "batch_size = 64\n",
    "eps_start = 0.9\n",
    "eps = eps_start\n",
    "eps_end = 0.05\n",
    "eps_decay = 10\n",
    "gamma = 0.999\n",
    "learning_rate = 0.01\n",
    "blind_prob = 0\n",
    "EXPLORE = 300\n",
    "\n",
    "pp = ProgressPlot(plot_names = ['Return', 'Exploration'], line_names = ['Value'])\n",
    "adrqn = ADRQN(n_actions, state_size, embedding_size).cuda()\n",
    "adrqn_target = ADRQN(n_actions, state_size, embedding_size).cuda()\n",
    "adrqn_target.load_state_dict(adrqn.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(adrqn.parameters(), lr = learning_rate)\n",
    "\n",
    "for i_episode in range(M_episodes):\n",
    "    now = time.time()\n",
    "    done = False\n",
    "    hidden = None\n",
    "    last_action = 0\n",
    "    current_return = 0\n",
    "    last_observation = env.reset()\n",
    "    for t in count():\n",
    "        \n",
    "        action, hidden = adrqn.act(torch.tensor(last_observation).float().view(1,1,-1).cuda(), F.one_hot(torch.tensor(last_action), n_actions).view(1,1,-1).float().cuda(), hidden = hidden, epsilon = eps)\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if np.random.rand() < blind_prob:\n",
    "            #Induce partial observability\n",
    "            observation = np.zeros_like(observation)\n",
    "\n",
    "        reward = np.sign(reward)\n",
    "        current_return += reward\n",
    "        replay_buffer.write_tuple((last_action, last_observation, action, reward, observation, done))\n",
    "        \n",
    "        last_action = action\n",
    "        last_observation = observation\n",
    "    \n",
    "        #Updating Networks\n",
    "        if i_episode > EXPLORE:\n",
    "                eps = eps_end + (eps_start - eps_end) * math.exp((-1*(i_episode-EXPLORE))/eps_decay)\n",
    "\n",
    "                last_actions, last_observations, actions, rewards, observations, dones = replay_buffer.sample(batch_size)\n",
    "                q_values, _ = adrqn.forward(last_observations, F.one_hot(last_actions, n_actions).float())\n",
    "                q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "                predicted_q_values, _ = adrqn_target.forward(observations, F.one_hot(actions, n_actions).float())\n",
    "                target_values = rewards + (gamma * (1 - dones.float()) * torch.max(predicted_q_values, dim = -1)[0])\n",
    "\n",
    "                #Update network parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss = torch.nn.MSELoss()(q_values , target_values.detach())\n",
    "                loss.backward()\n",
    "                optimizer.step()      \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    pp.update([[current_return],[eps]])\n",
    "    adrqn_target.load_state_dict(adrqn.state_dict())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Experience Buffer Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpBuffer2():\n",
    "    #Alternative Experience Buffer that stores sequences of fixed length\n",
    "    def __init__(self, max_seqs, seq_len):\n",
    "        self.max_seqs = max_seqs\n",
    "        self.counter = 0\n",
    "        self.seq_len = seq_len\n",
    "        self.storage = [[] for i in range(max_seqs)]\n",
    "\n",
    "    def write_tuple(self, aoaro):\n",
    "        if len(self.storage[self.counter]) >= self.seq_len:\n",
    "            self.counter += 1\n",
    "        self.storage[self.counter].append(aoaro)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #Sample batches of (action, observation, action, reward, observation, done) tuples\n",
    "        #With dimensions (batch_size, seq_len) for rewards/actions/done and (batch_size, seq_len, obs_dim) for observations\n",
    "        last_actions = []\n",
    "        last_observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        observations = []\n",
    "        dones = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            seq_idx = np.random.randint(self.counter)\n",
    "            last_act, last_obs, act, rew, obs, done = zip(*self.storage[seq_idx])\n",
    "            last_actions.append(list(last_act))\n",
    "            last_observations.append(last_obs)\n",
    "            actions.append(list(act))\n",
    "            rewards.append(list(rew))\n",
    "            observations.append(list(obs))\n",
    "            dones.append(list(done))\n",
    "           \n",
    "        return torch.tensor(last_actions).cuda(), torch.tensor(last_observations, dtype = torch.float32).cuda(), torch.tensor(actions).cuda(), torch.tensor(rewards).float().cuda() , torch.tensor(observations, dtype = torch.float32).cuda(), torch.tensor(dones).cuda()\n",
    "\n",
    "class ExpBuffer3():\n",
    "    #Alternative Experience Buffer that restricts sequence samples to specific episodes\n",
    "    def __init__(self, max_episodes):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.current_episode = -1\n",
    "        self.filled_episodes = 0\n",
    "        self.storage = [[] for i in range(max_episodes)]\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.current_episode += 1\n",
    "        if self.filled_episodes < self.max_episodes:\n",
    "            self.filled_episodes += 1\n",
    "        if self.current_episode == self.max_episodes:\n",
    "            self.current_episode = 0\n",
    "            self.storage[self.current_episode] = []\n",
    "        else:\n",
    "            self.storage[self.current_episode] = []\n",
    "\n",
    "    \n",
    "    def write_tuple(self, aoaro):\n",
    "        self.storage[self.current_episode].append(aoaro)\n",
    "    \n",
    "    def sample(self, batch_size, seq_len):\n",
    "        #Sample batches of (action, observation, action, reward, observation, done) tuples\n",
    "        #With dimensions (batch_size, seq_len) for rewards/actions/done and (batch_size, seq_len, obs_dim) for observations\n",
    "        last_actions = []\n",
    "        last_observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        observations = []\n",
    "        dones = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            while True:\n",
    "                episode_idx = np.random.randint(self.filled_episodes)\n",
    "                if len(self.storage[episode_idx]) >= seq_len:\n",
    "                    start_idx = np.random.randint(len(self.storage[episode_idx]) - seq_len + 1)\n",
    "                    last_act, last_obs, act, rew, obs, done = zip(*self.storage[episode_idx][start_idx:start_idx+ seq_len])\n",
    "                    last_actions.append(list(last_act))\n",
    "                    last_observations.append(last_obs)\n",
    "                    actions.append(list(act))\n",
    "                    rewards.append(list(rew))\n",
    "                    observations.append(list(obs))\n",
    "                    dones.append(list(done))\n",
    "                    break\n",
    "        \n",
    "        return torch.tensor(last_actions), torch.tensor(last_observations, dtype = torch.float32), torch.tensor(actions), torch.tensor(rewards).float() , torch.tensor(observations, dtype = torch.float32), torch.tensor(dones)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
